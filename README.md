# Multi-Threaded-Broken-Links-Finder-Using-Web-Crawling-
It is a Multithreaded Web Crawler which sifts through all the links present in a given website's home/main URL, and detects all the broken and non functional links and gives out the list of these links in the form of a text file. Three text files are created within a folder that is named by the user. The three text files are broken, queue and working, where the broken.txt has the broken links' URL in it, the queue.txt has the files that are in queue and yet to be crawled or sifted through, and the working.txt includes all the crawled or sifted functional links or URLs. The main.py is the main file and it takes the user input, so run the main.py file and provide the required input.
